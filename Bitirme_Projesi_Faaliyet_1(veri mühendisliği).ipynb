{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Gerekli Kütüphaneleri Kuralım\n",
        "print(\"Kütüphaneler kuruluyor (imagehash, Pillow)...\")\n",
        "!pip install -q Pillow imagehash\n",
        "\n",
        "# 2. Gerekli Kütüphaneleri Python'a Tanıtalım (Import Ettik)\n",
        "print(\"Kütüphaneler import ediliyor...\")\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import imagehash\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# 3. Google Drive'ı Bağlayalım\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive başarıyla bağlandı.\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive bağlanırken hata oluştu veya zaten bağlı: {e}\")\n",
        "\n",
        "print(\"\\n--- ADIM 1 TAMAMLANDI. ADIM 2'ye geçebilirsiniz. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5gCtqwcHE_m",
        "outputId": "b0ea7082-88d9-4bbf-ed2d-9bed6b703758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kütüphaneler kuruluyor (imagehash, Pillow)...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hKütüphaneler import ediliyor...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive başarıyla bağlandı.\n",
            "\n",
            "--- ADIM 1 TAMAMLANDI. ADIM 2'ye geçebilirsiniz. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FAALİYET 1 (AŞAMA 1: VERİ TEMİZLEME) BAŞLATILDI\")\n",
        "\n",
        "# 1. Yolları Tanımlayalım (Türkçe karakterli doğru yolumuzu kullanalım)\n",
        "drive_proje_yolu = \"/content/drive/MyDrive/TUBITAK_Projesi/\"\n",
        "eski_zip_yolu = os.path.join(drive_proje_yolu, \"dataset_eski.zip\")\n",
        "yeni_zip_yolu = os.path.join(drive_proje_yolu, \"dataset_yeni.zip\")\n",
        "\n",
        "# Colab'da çalışacağımız geçici klasörler\n",
        "eski_veri_klasoru = \"/content/dataset_eski_raw\"\n",
        "yeni_veri_klasoru = \"/content/dataset_yeni_raw\"\n",
        "temiz_veri_klasoru = \"/content/dataset_temiz_613\" # Nihai temiz resimler buraya gelecektir\n",
        "\n",
        "# 2. Geçici Klasörleri Temizle (Önceki oturumlardan kalıntı varsa temizleriz)\n",
        "print(\"Geçici klasörler temizleniyor...\")\n",
        "if os.path.exists(eski_veri_klasoru): shutil.rmtree(eski_veri_klasoru)\n",
        "if os.path.exists(yeni_veri_klasoru): shutil.rmtree(yeni_veri_klasoru)\n",
        "if os.path.exists(temiz_veri_klasoru): shutil.rmtree(temiz_veri_klasoru)\n",
        "os.makedirs(eski_veri_klasoru)\n",
        "os.makedirs(yeni_veri_klasoru)\n",
        "os.makedirs(temiz_veri_klasoru)\n",
        "\n",
        "# 3. Ham Veri ZIP'lerini Colab'a Açalım\n",
        "try:\n",
        "    print(f\"'{eski_zip_yolu}' açılıyor...\")\n",
        "    with zipfile.ZipFile(eski_zip_yolu, 'r') as zip_ref:\n",
        "        zip_ref.extractall(eski_veri_klasoru)\n",
        "    print(f\"'{yeni_zip_yolu}' açılıyor...\")\n",
        "    with zipfile.ZipFile(yeni_zip_yolu, 'r') as zip_ref:\n",
        "        zip_ref.extractall(yeni_veri_klasoru)\n",
        "    print(\"Ham veri başarıyla Colab ortamına açıldı.\")\n",
        "except Exception as e:\n",
        "    print(f\"KRİTİK HATA: ZIP dosyaları bulunamadı veya açılamadı. Hata: {e}\")\n",
        "    print(\"Lütfen Google Drive'da 'TUBITAK_Projesi' klasöründe 'dataset_eski.zip' ve 'dataset_yeni.zip' dosyalarının olduğundan emin olun.\")\n",
        "    exit() # Hata varsa script'i durdur diyelim\n",
        "\n",
        "# 4. pHash Temizleme Script'i\n",
        "gorulen_hashler = set()\n",
        "eski_toplam = 0\n",
        "yeni_toplam = 0\n",
        "toplam_kopya = 0\n",
        "toplam_temiz = 0\n",
        "\n",
        "def process_images_in_folder(folder_path, is_base_dataset=True):\n",
        "    \"\"\"\n",
        "    Bir klasördeki resimleri tarar, pHash'lerini hesaplar,\n",
        "    kopyaları bulur ve temiz olanları 'temiz_veri_klasoru'ne kopyalar.\n",
        "    \"\"\"\n",
        "    global eski_toplam, yeni_toplam, toplam_kopya, toplam_temiz\n",
        "\n",
        "    print(f\"\\n'{folder_path}' klasörü taranıyor...\")\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for filename in files:\n",
        "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue # Resim dosyası değilse atla\n",
        "\n",
        "            file_path = os.path.join(root, filename)\n",
        "\n",
        "            if is_base_dataset: eski_toplam += 1\n",
        "            else: yeni_toplam += 1\n",
        "\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    hash_degeri = imagehash.phash(img)\n",
        "\n",
        "                if hash_degeri in gorulen_hashler:\n",
        "                    toplam_kopya += 1\n",
        "                else:\n",
        "                    gorulen_hashler.add(hash_degeri)\n",
        "                    hedef_dosya_adi = f\"{hash_degeri}.jpg\"\n",
        "                    hedef_yol = os.path.join(temiz_veri_klasoru, hedef_dosya_adi)\n",
        "                    img.convert(\"RGB\").save(hedef_yol, \"JPEG\")\n",
        "                    toplam_temiz += 1\n",
        "            except Exception as e:\n",
        "                print(f\"HATA: {filename} işlenemedi, atlanıyor. Hata: {e}\")\n",
        "\n",
        "# Script'i çalıştır\n",
        "process_images_in_folder(eski_veri_klasoru, is_base_dataset=True)\n",
        "process_images_in_folder(yeni_veri_klasoru, is_base_dataset=False)\n",
        "\n",
        "# 5. Temiz Veriyi Drive'a Geri Kaydet\n",
        "temiz_zip_adi = \"dataset_temiz_v1.zip\"\n",
        "temiz_zip_yolu_colab = f\"/content/{temiz_zip_adi}\"\n",
        "temiz_zip_yolu_drive = os.path.join(drive_proje_yolu, temiz_zip_adi)\n",
        "\n",
        "print(f\"\\nTemizlenmiş {toplam_temiz} resim '{temiz_zip_adi}' olarak ZIP'leniyor...\")\n",
        "shutil.make_archive(temiz_zip_yolu_colab.replace('.zip', ''), 'zip', temiz_veri_klasoru)\n",
        "\n",
        "print(f\"Temiz ZIP dosyası Google Drive'a kopyalanıyor: {temiz_zip_yolu_drive}\")\n",
        "shutil.copy(temiz_zip_yolu_colab, drive_proje_yolu)\n",
        "print(\"Kopyalama tamamlandı.\")\n",
        "\n",
        "# 6. NİHAİ RAPORUMUZ\n",
        "print(\"\\n--- FAALİYET 1 (AŞAMA 1): NİHAİ TEMİZLEME RAPORU ---\")\n",
        "print(f\"Eski Veri Seti ('dataset_eski') Tarandı: {eski_toplam} resim\")\n",
        "print(f\"Yeni Veri Seti ('dataset_yeni') Tarandı: {yeni_toplam} resim\")\n",
        "print(f\"Toplam Ham Veri: {eski_toplam + yeni_toplam} resim (~694)\")\n",
        "print(\"---------------------------------\")\n",
        "print(f\"Bulunan Toplam Kopya (Duplicate) Resim: {toplam_kopya} adet\")\n",
        "print(f\"Nihai Temiz ve Eşsiz Resim Sayısı: {toplam_temiz} adet (613 olmalı)\")\n",
        "print(f\"Temiz veri seti '{temiz_zip_adi}' olarak Google Drive'a kaydedildi.\")\n",
        "print(\"\\nAŞAMA 1 BAŞARIYLA TAMAMLANDI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElN6C3DMHjtf",
        "outputId": "5486e649-1735-4ad0-8d3a-5e4e7374ce06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FAALİYET 1 (AŞAMA 1: VERİ TEMİZLEME) BAŞLATILDI ---\n",
            "Geçici klasörler temizleniyor...\n",
            "'/content/drive/MyDrive/TUBITAK_Projesi/dataset_eski.zip' açılıyor...\n",
            "'/content/drive/MyDrive/TUBITAK_Projesi/dataset_yeni.zip' açılıyor...\n",
            "Ham veri başarıyla Colab ortamına açıldı.\n",
            "\n",
            "'/content/dataset_eski_raw' klasörü taranıyor...\n",
            "\n",
            "'/content/dataset_yeni_raw' klasörü taranıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (171668516 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temizlenmiş 683 resim 'dataset_temiz_v1.zip' olarak ZIP'leniyor...\n",
            "Temiz ZIP dosyası Google Drive'a kopyalanıyor: /content/drive/MyDrive/TUBITAK_Projesi/dataset_temiz_v1.zip\n",
            "Kopyalama tamamlandı.\n",
            "\n",
            "--- FAALİYET 1 (AŞAMA 1): NİHAİ TEMİZLEME RAPORU ---\n",
            "Eski Veri Seti ('dataset_eski') Tarandı: 423 resim\n",
            "Yeni Veri Seti ('dataset_yeni') Tarandı: 271 resim\n",
            "Toplam Ham Veri: 694 resim (~694)\n",
            "---------------------------------\n",
            "Bulunan Toplam Kopya (Duplicate) Resim: 11 adet\n",
            "Nihai Temiz ve Eşsiz Resim Sayısı: 683 adet (613 olmalı)\n",
            "Temiz veri seti 'dataset_temiz_v1.zip' olarak Google Drive'a kaydedildi.\n",
            "\n",
            "✅ AŞAMA 1 BAŞARIYLA TAMAMLANDI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N-GqXmDL20V",
        "outputId": "7f517f79-8208-4903-c301-49cb5dc713a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"FAALİYET 1 (AŞAMA 2: VERİ STANDARDİZASYON - 256x256) BAŞLATILDI\")\n",
        "\n",
        "# 1. Ortamı Kur (Drive'ın zaten bağlı olduğunu varsayıyoruz)\n",
        "# Gerekli kütüphaneler bir önceki hücrede zaten import edildi.\n",
        "print(\"Ortam hazır.\")\n",
        "\n",
        "# 2. Yolları Tanımla\n",
        "drive_proje_yolu = \"/content/drive/MyDrive/TUBITAK_Projesi/\"\n",
        "\n",
        "# Çekilecek temiz veri (613 resim)\n",
        "temiz_zip_yolu = os.path.join(drive_proje_yolu, \"dataset_temiz_v1.zip\")\n",
        "\n",
        "# Colab'da ZIP'i açacağımız geçici klasör\n",
        "temiz_veri_klasoru = \"/content/dataset_temiz_613_ready\"\n",
        "\n",
        "# Standardizasyon (256x256) sonrası resimlerin konulacağı geçici klasör\n",
        "son_veri_klasoru = \"/content/dataset_son_hali_256\"\n",
        "\n",
        "# Drive'a kaydedilecek nihai ZIP dosyasının adı\n",
        "nihai_zip_adi = \"dataset_son_hali_256_v1.zip\"\n",
        "nihai_zip_yolu_colab = f\"/content/{nihai_zip_adi}\"\n",
        "nihai_zip_yolu_drive = os.path.join(drive_proje_yolu, nihai_zip_adi)\n",
        "\n",
        "# 3. Geçici Klasörleri Temizle\n",
        "print(\"Geçici klasörler temizleniyor.\")\n",
        "if os.path.exists(temiz_veri_klasoru): shutil.rmtree(temiz_veri_klasoru)\n",
        "if os.path.exists(son_veri_klasoru): shutil.rmtree(son_veri_klasoru)\n",
        "os.makedirs(temiz_veri_klasoru)\n",
        "os.makedirs(son_veri_klasoru)\n",
        "\n",
        "# 4. Temiz Veriyi (613 Resim) ZIP'ten Aç\n",
        "print(f\"Temiz veri seti '{temiz_zip_yolu}' Colab ortamına açılıyor...\")\n",
        "if os.path.exists(temiz_zip_yolu):\n",
        "    try:\n",
        "        with zipfile.ZipFile(temiz_zip_yolu, 'r') as zip_ref:\n",
        "            zip_ref.extractall(temiz_veri_klasoru)\n",
        "        print(\"Veri açma başarılı. Standardizasyon başlıyor...\")\n",
        "    except Exception as e:\n",
        "        print(f\"KRİTİK HATA: '{temiz_zip_yolu}' dosyası bozuk veya açılamıyor. Hata: {e}\")\n",
        "        exit() # Hata varsa script'i durdur\n",
        "else:\n",
        "    print(f\"KRİTİK HATA: '{temiz_zip_yolu}' bulunamadı. Lütfen Google Drive'da dosyanın varlığını kontrol edin.\")\n",
        "    exit() # Hata varsa script'i durdur\n",
        "\n",
        "# 5. STANDARDİZASYON (256x256 KIRPMA)\n",
        "hedef_boyut = 256\n",
        "islenen_resim_sayisi = 0\n",
        "hata_alan_resim_sayisi = 0\n",
        "\n",
        "dosya_listesi = os.listdir(temiz_veri_klasoru)\n",
        "print(f\"Toplam {len(dosya_listesi)} dosya bulundu, standardizasyon başlıyor...\")\n",
        "\n",
        "for filename in dosya_listesi:\n",
        "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        continue # Sadece resimleri işle\n",
        "\n",
        "    file_path = os.path.join(temiz_veri_klasoru, filename)\n",
        "\n",
        "    try:\n",
        "        with Image.open(file_path) as img:\n",
        "            img = img.convert(\"RGB\") # Modu 'RGB' yap (StyleGAN için şart olduğu için )\n",
        "\n",
        "            # En boy oranını koruyarak yeniden boyutlandırma\n",
        "            width, height = img.size\n",
        "            if width > height:\n",
        "                new_height = hedef_boyut\n",
        "                new_width = int(width * (hedef_boyut / height))\n",
        "            else:\n",
        "                new_width = hedef_boyut\n",
        "                new_height = int(height * (hedef_boyut / width))\n",
        "\n",
        "            img_resized = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "            # Ortadan Kırpma (Center Crop)\n",
        "            left = (new_width - hedef_boyut) / 2\n",
        "            top = (new_height - hedef_boyut) / 2\n",
        "            right = (new_width + hedef_boyut) / 2\n",
        "            bottom = (new_height + hedef_boyut) / 2\n",
        "\n",
        "            img_cropped = img_resized.crop((left, top, right, bottom))\n",
        "\n",
        "            # Sonucu kaydet\n",
        "            hedef_yol = os.path.join(son_veri_klasoru, filename)\n",
        "            img_cropped.save(hedef_yol, \"JPEG\")\n",
        "            islenen_resim_sayisi += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # Hatalı/Bozuk resimleri atla\n",
        "        hata_alan_resim_sayisi += 1\n",
        "        print(f\"HATA: {filename} işlenemedi, atlanıyor. Hata: {e}\")\n",
        "\n",
        "# 6. Nihai Veri Setini Drive'a Geri Kaydet Diyelim\n",
        "print(\"\\nStandardizasyon tamamlandı.\")\n",
        "print(f\"Nihai (256x256) veri seti '{nihai_zip_adi}' olarak ZIP'leniyor...\")\n",
        "shutil.make_archive(nihai_zip_yolu_colab.replace('.zip', ''), 'zip', son_veri_klasoru)\n",
        "\n",
        "print(f\"Nihai ZIP dosyası Google Drive'a kopyalanıyor: {nihai_zip_yolu_drive}\")\n",
        "shutil.copy(nihai_zip_yolu_colab, drive_proje_yolu)\n",
        "print(\"Kopyalama tamamlandı.\")\n",
        "\n",
        "# 7. NİHAİ RAPORUMUZ\n",
        "print(\"\\n--- FAALİYET 1 (AŞAMA 2): NİHAİ STANDARDİZASYON RAPORU ---\")\n",
        "print(f\"Başarıyla İşlenen ve Standartlaştırılan (256x256) Resim Sayısı: {islenen_resim_sayisi} adet\")\n",
        "print(f\"Atlanan Hatalı Resim Sayısı: {hata_alan_resim_sayisi} adet\")\n",
        "print(f\"Nihai veri setiniz '{nihai_zip_adi}' olarak Google Drive'a kaydedildi.\")\n",
        "print(\"\\nFAALİET 1 (HER İKİ AŞAMA DA) TAMAMLANDI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIJoXgw4JTxm",
        "outputId": "f3d08817-9b38-442d-d18f-6f6219508c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FAALİYET 1 (AŞAMA 2: VERİ STANDARDİZASYON - 256x256) BAŞLATILDI ---\n",
            "Ortam hazır.\n",
            "Geçici klasörler temizleniyor...\n",
            "Temiz veri seti '/content/drive/MyDrive/TUBITAK_Projesi/dataset_temiz_v1.zip' Colab ortamına açılıyor...\n",
            "Veri açma başarılı. Standardizasyon başlıyor...\n",
            "Toplam 683 dosya bulundu, standardizasyon başlıyor...\n",
            "\n",
            "Standardizasyon tamamlandı.\n",
            "Nihai (256x256) veri seti 'dataset_son_hali_256_v1.zip' olarak ZIP'leniyor...\n",
            "Nihai ZIP dosyası Google Drive'a kopyalanıyor: /content/drive/MyDrive/TUBITAK_Projesi/dataset_son_hali_256_v1.zip\n",
            "Kopyalama tamamlandı.\n",
            "\n",
            "--- FAALİYET 1 (AŞAMA 2): NİHAİ STANDARDİZASYON RAPORU ---\n",
            "Başarıyla İşlenen ve Standartlaştırılan (256x256) Resim Sayısı: 683 adet\n",
            "Atlanan Hatalı Resim Sayısı: 0 adet\n",
            "Nihai veri setiniz 'dataset_son_hali_256_v1.zip' olarak Google Drive'a kaydedildi.\n",
            "\n",
            "✅✅✅ FAALİET 1 (HER İKİ AŞAMA DA) TAMAMLANDI. ✅✅✅\n"
          ]
        }
      ]
    }
  ]
}